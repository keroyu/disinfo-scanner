# Data Model: Video Comment Density Analysis

**Feature**: `008-video-comment-density`
**Date**: 2025-11-19
**Phase**: 1 (Design & Contracts)

## Overview

This document defines the data structures, transformations, and relationships for the comment density analysis feature. The feature reuses existing database entities (Video, Comment) and introduces new request/response data transfer objects for the API layer.

---

## Existing Database Entities

### Video (Existing - No Schema Changes)

**Table**: `videos`

**Primary Key**: `video_id` (string, non-incrementing)

**Attributes**:
- `video_id` (string, PK) - YouTube video identifier
- `channel_id` (string, FK) - References channels table
- `title` (string) - Video title
- `published_at` (timestamp, UTC) - Video publication time
- `comment_count` (integer) - Metadata comment count from YouTube API
- `created_at` (timestamp, UTC) - Laravel timestamp
- `updated_at` (timestamp, UTC) - Laravel timestamp

**Relationships**:
- `belongsTo(Channel)` - Each video belongs to one channel
- `hasMany(Comment)` - Each video has many comments

**New Methods** (to be added to Video model):
```php
/**
 * Get the URL for the video analysis page
 *
 * @return string
 */
public function analysisUrl(): string
{
    return route('videos.analysis', ['video' => $this->video_id]);
}
```

**Validation Rules**: No changes (existing validation sufficient)

---

### Comment (Existing - No Schema Changes)

**Table**: `comments`

**Primary Key**: `comment_id` (string, non-incrementing)

**Attributes**:
- `comment_id` (string, PK) - YouTube comment identifier
- `video_id` (string, FK) - References videos table
- `author_channel_id` (string, FK) - References authors table
- `text` (text) - Comment content
- `like_count` (integer) - Number of likes
- `published_at` (timestamp, UTC) - Comment publication time **[CRITICAL FOR FEATURE]**
- `parent_comment_id` (string, nullable, FK) - For reply comments
- `created_at` (timestamp, UTC) - Laravel timestamp
- `updated_at` (timestamp, UTC) - Laravel timestamp

**Relationships**:
- `belongsTo(Video)` - Each comment belongs to one video
- `belongsTo(Author)` - Each comment has one author
- `belongsTo(Comment, 'parent_comment_id')` - Parent comment for replies
- `hasMany(Comment, 'parent_comment_id')` - Child reply comments

**Required Index** (verify exists or add migration):
```sql
CREATE INDEX idx_video_published ON comments (video_id, published_at);
```

**Validation Rules**: No changes (existing validation sufficient)

---

## New Data Transfer Objects

### VideoAnalysisRequest

**Purpose**: Simplified API request (no range parameters needed - server returns all data)

**Attributes**:
- `video_id` (string, required) - Target video identifier (from URL path)

**Validation Rules**:
```php
[
    'video_id' => 'required|string|exists:videos,video_id',
]
```

**Note**: Unlike the original design, this optimized version does NOT require `range_type`, `custom_start_date`, or `custom_end_date` parameters. The API always returns both complete datasets (hourly + daily), and the frontend performs filtering based on user selection.

**State Transitions**: None (stateless DTO)

---

### CommentDensityDataPoint

**Purpose**: Represent a single time bucket with comment count

**Attributes**:
- `timestamp` (string, ISO 8601) - Time bucket start in Asia/Taipei timezone
- `display_time` (string) - Human-readable time with timezone label (e.g., "2025-11-19 14:00 (GMT+8)")
- `count` (integer) - Number of comments in this time bucket
- `bucket_size` (string) - Either "hour" or "day"

**Example**:
```json
{
  "timestamp": "2025-11-19T14:00:00+08:00",
  "display_time": "2025-11-19 14:00 (GMT+8)",
  "count": 127,
  "bucket_size": "hour"
}
```

**Validation Rules**: None (internal DTO, always generated by service)

---

### CommentDensityResponse

**Purpose**: API response containing TWO complete datasets (hourly + daily) for client-side filtering

**Design Rationale**:
- **Performance optimization**: One-time data fetch instead of multiple queries per user interaction
- **User experience**: Instant range switching without network latency (0ms vs 500-3000ms)
- **Database load reduction**: 2 queries total (hourly + daily) instead of N queries (one per range selection)
- **Data size**: ~5-10 KB JSON (336 hourly + ~100 daily data points) - trivial for modern browsers

**Attributes**:
- `video_id` (string) - Target video identifier
- `video_title` (string) - Video title for context
- `video_published_at` (string, ISO 8601 GMT+8) - Video publication time
- `hourly_data` (object) - Complete hourly dataset for first 14 days after publication
  - `range` (object) - Time range covered by this dataset
    - `start` (string, ISO 8601 GMT+8) - Video publication time
    - `end` (string, ISO 8601 GMT+8) - 14 days after publication OR current time (whichever is earlier)
    - `total_hours` (integer) - Number of hours in range
  - `data` (array of CommentDensityDataPoint) - Dense hourly time-series (includes zero-count buckets)
- `daily_data` (object) - Complete daily dataset from publication to current date
  - `range` (object) - Time range covered by this dataset
    - `start` (string, ISO 8601 GMT+8) - Video publication date (start of day)
    - `end` (string, ISO 8601 GMT+8) - Current date (start of day)
    - `total_days` (integer) - Number of days in range
  - `data` (array of CommentDensityDataPoint) - Dense daily time-series (includes zero-count buckets)
- `metadata` (object) - Performance and debugging info
  - `query_time_ms` (integer) - Total database query execution time (both queries combined)
  - `hourly_data_points` (integer) - Number of hourly buckets returned
  - `daily_data_points` (integer) - Number of daily buckets returned
  - `trace_id` (string, UUID) - Unique identifier for this request

**Example** (video published 2025-10-10 15:00, current date 2025-11-19):
```json
{
  "video_id": "abc123xyz",
  "video_title": "Sample Video Title",
  "video_published_at": "2025-10-10T15:00:00+08:00",

  "hourly_data": {
    "range": {
      "start": "2025-10-10T15:00:00+08:00",
      "end": "2025-10-24T15:00:00+08:00",
      "total_hours": 336
    },
    "data": [
      {
        "timestamp": "2025-10-10T15:00:00+08:00",
        "display_time": "2025-10-10 15:00 (GMT+8)",
        "count": 45,
        "bucket_size": "hour"
      },
      {
        "timestamp": "2025-10-10T16:00:00+08:00",
        "display_time": "2025-10-10 16:00 (GMT+8)",
        "count": 23,
        "bucket_size": "hour"
      },
      {
        "timestamp": "2025-10-10T17:00:00+08:00",
        "display_time": "2025-10-10 17:00 (GMT+8)",
        "count": 0,
        "bucket_size": "hour"
      }
      // ... total 336 data points (14 days × 24 hours)
    ]
  },

  "daily_data": {
    "range": {
      "start": "2025-10-10T00:00:00+08:00",
      "end": "2025-11-19T00:00:00+08:00",
      "total_days": 40
    },
    "data": [
      {
        "timestamp": "2025-10-10T00:00:00+08:00",
        "display_time": "2025-10-10 00:00 (GMT+8)",
        "count": 320,
        "bucket_size": "day"
      },
      {
        "timestamp": "2025-10-11T00:00:00+08:00",
        "display_time": "2025-10-11 00:00 (GMT+8)",
        "count": 187,
        "bucket_size": "day"
      },
      {
        "timestamp": "2025-10-12T00:00:00+08:00",
        "display_time": "2025-10-12 00:00 (GMT+8)",
        "count": 0,
        "bucket_size": "day"
      }
      // ... total 40 data points
    ]
  },

  "metadata": {
    "query_time_ms": 1284,
    "hourly_data_points": 336,
    "daily_data_points": 40,
    "trace_id": "550e8400-e29b-41d4-a716-446655440000"
  }
}
```

**Client-Side Filtering Logic**:
```javascript
// Frontend filters data based on user selection (no additional API calls)
function getFilteredData(cachedResponse, rangeType, customStart, customEnd) {
  switch (rangeType) {
    case '3days':   // First 72 hours
      return cachedResponse.hourly_data.data.slice(0, 72);
    case '7days':   // First 168 hours
      return cachedResponse.hourly_data.data.slice(0, 168);
    case '14days':  // All 336 hours
      return cachedResponse.hourly_data.data;
    case '30days':  // First 30 days
      return cachedResponse.daily_data.data.slice(0, 30);
    case 'custom':
      const daysDiff = (customEnd - customStart) / (1000*60*60*24);
      if (daysDiff <= 14) {
        // Use hourly data, filter by timestamp
        return cachedResponse.hourly_data.data.filter(d =>
          new Date(d.timestamp) >= customStart && new Date(d.timestamp) <= customEnd
        );
      } else {
        // Use daily data, filter by timestamp
        return cachedResponse.daily_data.data.filter(d =>
          new Date(d.timestamp) >= customStart && new Date(d.timestamp) <= customEnd
        );
      }
  }
}
```

**Validation Rules**: None (server-generated response)

---

### CommentDensityErrorResponse

**Purpose**: Structured error response with technical details (per FR-017 clarification)

**Attributes**:
- `error` (object) - Error details
  - `type` (string) - Error category (e.g., "DatabaseQueryException", "ValidationException", "TimeoutException")
  - `message` (string) - Human-readable error message
  - `details` (object, optional) - Technical debugging information
    - `trace_id` (string, UUID) - Links to server logs
    - `sql` (string, optional) - Failed SQL query (if database error)
    - `parameters` (object, optional) - Query parameters that caused failure
    - `timestamp` (string, ISO 8601) - When error occurred

**Example**:
```json
{
  "error": {
    "type": "DatabaseQueryException",
    "message": "Database query failed: Connection timeout after 30 seconds",
    "details": {
      "trace_id": "550e8400-e29b-41d4-a716-446655440000",
      "sql": "SELECT DATE_FORMAT(...) FROM comments WHERE video_id = ? ...",
      "parameters": {
        "video_id": "abc123xyz",
        "start_date": "2025-11-15 10:00:00",
        "end_date": "2025-11-18 10:00:00"
      },
      "timestamp": "2025-11-19T15:30:45+08:00"
    }
  }
}
```

**Validation Rules**: None (server-generated error)

---

## Data Transformations

### Optimized Dual-Dataset Aggregation

**Design Philosophy**: Query both hourly and daily datasets in a single API call, enabling client-side filtering without additional server requests.

---

### SQL Query Strategy

**Query 1: Hourly Aggregation (First 14 Days)**

**Input**: video_id, video_published_at
**Output**: Array of hourly buckets from publication time to (publication + 14 days) OR current time

```sql
SELECT
  DATE_FORMAT(
    CONVERT_TZ(published_at, '+00:00', '+08:00'),
    '%Y-%m-%d %H:00:00'
  ) as time_bucket,
  COUNT(*) as comment_count
FROM comments
WHERE video_id = :video_id
  AND published_at >= :video_published_at_utc
  AND published_at <= :hourly_end_date_utc
GROUP BY time_bucket
ORDER BY time_bucket ASC
```

**Parameters**:
- `:video_id` - Video identifier
- `:video_published_at_utc` - Video publication time in UTC
- `:hourly_end_date_utc` - Min(video_published_at + 14 days, current_time) in UTC

**Expected Result Size**: Up to 336 rows (14 days × 24 hours)

---

**Query 2: Daily Aggregation (Publication to Current Date)**

**Input**: video_id, video_published_at
**Output**: Array of daily buckets from publication date to current date

```sql
SELECT
  DATE(
    CONVERT_TZ(published_at, '+00:00', '+08:00')
  ) as time_bucket,
  COUNT(*) as comment_count
FROM comments
WHERE video_id = :video_id
  AND published_at >= :video_published_at_utc
  AND published_at <= :current_time_utc
GROUP BY time_bucket
ORDER BY time_bucket ASC
```

**Parameters**:
- `:video_id` - Video identifier
- `:video_published_at_utc` - Video publication time in UTC (start of day)
- `:current_time_utc` - Current timestamp in UTC

**Expected Result Size**: Variable (depends on video age, e.g., 40 days = 40 rows, 100 days = 100 rows)

---

**Combined Query Performance**:
- Total queries: 2 (instead of N per user interaction)
- Expected total query time: <1.5 seconds for typical datasets
- Index requirement: `idx_video_published (video_id, published_at)` - **CRITICAL**

---

### Missing Bucket Filling

**Input**: Sparse array of time buckets (from SQL)
**Output**: Dense array with zero-count buckets for missing time periods

**Logic**:
```php
public function fillMissingBuckets(
    array $sparseData,
    Carbon $start,
    Carbon $end,
    string $granularity
): array {
    $denseBuckets = [];
    $existingBuckets = collect($sparseData)->keyBy('time_bucket');

    $current = $start->copy();
    $increment = ($granularity === 'hourly') ? 'addHour' : 'addDay';
    $format = ($granularity === 'hourly') ? 'Y-m-d H:00:00' : 'Y-m-d';

    while ($current->lessThanOrEqualTo($end)) {
        $bucketKey = $current->format($format);

        if ($existingBuckets->has($bucketKey)) {
            $denseBuckets[] = $existingBuckets[$bucketKey];
        } else {
            $denseBuckets[] = [
                'time_bucket' => $bucketKey,
                'comment_count' => 0
            ];
        }

        $current->$increment();
    }

    return $denseBuckets;
}
```

**Rationale**: Ensures Chart.js displays continuous time series with no visual gaps

---

### Timezone Conversion (UTC → GMT+8)

**Input**: Dense array of time buckets with UTC-based keys
**Output**: Array of CommentDensityDataPoint objects with GMT+8 timestamps

**Logic**:
```php
public function convertToDataPoints(array $denseBuckets, string $granularity): array
{
    return array_map(function($bucket) use ($granularity) {
        $taipeiTime = Carbon::parse($bucket['time_bucket'], 'Asia/Taipei');

        return [
            'timestamp' => $taipeiTime->toIso8601String(),
            'display_time' => $taipeiTime->format('Y-m-d H:i') . ' (GMT+8)',
            'count' => (int) $bucket['comment_count'],
            'bucket_size' => $granularity === 'hourly' ? 'hour' : 'day'
        ];
    }, $denseBuckets);
}
```

---

## Data Flow Diagram (Optimized)

### Backend Flow (One-Time Data Fetch)

```
User clicks "分析" button
  ↓
Frontend sends GET /api/videos/{video_id}/comment-density
  ↓
Controller validates video_id exists
  ↓
Service: Load video model → Get published_at timestamp
  ↓
┌─────────────────────────────────────────────────────────┐
│  Parallel Data Aggregation (2 queries executed)        │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  Query 1: Hourly Data                                  │
│  - Range: published_at to (published_at + 14 days)     │
│  - Buckets: 1-hour intervals                           │
│  - Result: Up to 336 sparse buckets                    │
│                                                         │
│  Query 2: Daily Data                                   │
│  - Range: published_at to current_date                 │
│  - Buckets: 1-day intervals                            │
│  - Result: N sparse buckets (N = days since publish)   │
│                                                         │
└─────────────────────────────────────────────────────────┘
  ↓
Service: fillMissingBuckets() for hourly data → 336 dense buckets
  ↓
Service: fillMissingBuckets() for daily data → N dense buckets
  ↓
Service: convertToDataPoints() → GMT+8 timestamps for both datasets
  ↓
Service: Build CommentDensityResponse with hourly_data + daily_data
  ↓
Controller returns JSON (single response, ~5-10 KB)
  ↓
Frontend caches response in memory
```

### Frontend Flow (Instant Range Switching)

```
User selects time range (e.g., "7 days")
  ↓
JavaScript filters cached data (NO API call):
  - If "3 days" → hourly_data.slice(0, 72)
  - If "7 days" → hourly_data.slice(0, 168)
  - If "14 days" → hourly_data (all 336 points)
  - If "30 days" → daily_data.slice(0, 30)
  - If custom ≤14 days → filter hourly_data by timestamp
  - If custom >14 days → filter daily_data by timestamp
  ↓
Chart.js re-renders with filtered data (instant, 0ms)
```

### Performance Comparison

| Action | Old Design | Optimized Design | Improvement |
|--------|-----------|------------------|-------------|
| Initial page load | 1 query (~500ms) | 2 queries (~1000ms) | Slightly slower |
| Switch to 7 days | 1 query (~500ms) | 0 queries (0ms) | ∞ faster |
| Switch to 14 days | 1 query (~500ms) | 0 queries (0ms) | ∞ faster |
| Switch to 30 days | 1 query (~800ms) | 0 queries (0ms) | ∞ faster |
| Custom range | 1 query (~500ms) | 0 queries (0ms) | ∞ faster |
| **Total for 5 switches** | **5 queries (~3000ms)** | **2 queries (~1000ms)** | **3x faster** |

---

## Validation Summary

| Entity/DTO | Validation Layer | Rules |
|------------|------------------|-------|
| Video | Model | Existing (no changes) |
| Comment | Model | Existing (no changes) |
| TimeRangeRequest | Controller (FormRequest) | range_type enum, custom_start_date/end_date required_if, date logic |
| CommentDensityDataPoint | None | Internal DTO, always valid by construction |
| CommentDensityResponse | None | Server-generated, structure guaranteed |
| CommentDensityErrorResponse | None | Server-generated, structure guaranteed |

---

## Index Requirements

**Verify or create**:
```sql
-- Critical for performance
CREATE INDEX idx_video_published ON comments (video_id, published_at);
```

**Rationale**: Composite index on video_id and published_at enables efficient range scans for time-based aggregation queries. Without this index, queries on large tables (100k+ comments) will perform full table scans and exceed the 3-second performance target.

---

## Next Steps

Proceed to create API contracts in `/contracts/` directory defining the OpenAPI specification for the comment density endpoint.
